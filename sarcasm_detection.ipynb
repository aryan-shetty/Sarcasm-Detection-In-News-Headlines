{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the first dataset\n",
        "df1 = pd.read_json('Sarcasm_Headlines_Dataset.json', lines=True)\n",
        "\n",
        "# Load the second dataset\n",
        "df2 = pd.read_json('Sarcasm_Headlines_Dataset_v2.json', lines=True)\n",
        "\n",
        "# Concatenate both datasets into a single dataframe\n",
        "df = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "# Calculate the total number of headlines\n",
        "total_headlines = len(df)\n",
        "\n",
        "# Calculate the percentage of sarcastic and non-sarcastic headlines\n",
        "num_sarcastic = df['is_sarcastic'].sum()\n",
        "num_non_sarcastic = total_headlines - num_sarcastic\n",
        "percentage_sarcastic = (num_sarcastic / total_headlines) * 100\n",
        "percentage_non_sarcastic = (num_non_sarcastic / total_headlines) * 100\n",
        "\n",
        "# Calculate the average headline length\n",
        "df['headline_length'] = df['headline'].apply(lambda x: len(x.split()))\n",
        "average_headline_length = df['headline_length'].mean()\n",
        "\n",
        "# Find the minimum and maximum headline lengths\n",
        "min_headline_length = df['headline_length'].min()\n",
        "max_headline_length = df['headline_length'].max()\n",
        "\n",
        "# Print the results\n",
        "print(\"Total number of headlines:\", total_headlines)\n",
        "print(\"Percentage of sarcastic headlines:\", percentage_sarcastic, \"%\")\n",
        "print(\"Percentage of non-sarcastic headlines:\", percentage_non_sarcastic, \"%\")\n",
        "print(\"Average headline length:\", average_headline_length, \"words\")\n",
        "print(\"Minimum headline length:\", min_headline_length, \"words\")\n",
        "print(\"Maximum headline length:\", max_headline_length, \"words\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQ1lTYfiAu9A",
        "outputId": "34c3e2eb-8b36-46bd-ff1e-f32589351f10"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of headlines: 55328\n",
            "Percentage of sarcastic headlines: 45.83212839791787 %\n",
            "Percentage of non-sarcastic headlines: 54.16787160208213 %\n",
            "Average headline length: 9.951417004048583 words\n",
            "Minimum headline length: 2 words\n",
            "Maximum headline length: 151 words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the datasets and combine them\n",
        "data_v1 = pd.read_json(\"Sarcasm_Headlines_Dataset.json\", lines=True)\n",
        "data_v2 = pd.read_json(\"Sarcasm_Headlines_Dataset_v2.json\", lines=True)\n",
        "data = pd.concat([data_v1, data_v2])\n",
        "\n",
        "# Data cleaning and preprocessing\n",
        "def clean_text(text):\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize and remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    filtered_text = ' '.join(word for word in words if word not in stop_words)\n",
        "\n",
        "    return filtered_text\n",
        "\n",
        "# Apply data cleaning function to 'headline' column\n",
        "data['headline'] = data['headline'].apply(clean_text)\n",
        "\n",
        "# Split the data into training and testing sets (80% training, 20% testing)\n",
        "X = data['headline']\n",
        "y = data['is_sarcastic']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Bag-of-Words representation of the headlines\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "X_test_bow = vectorizer.transform(X_test)\n",
        "\n",
        "# Train the Multinomial Naive Bayes classifier\n",
        "clf = MultinomialNB(alpha=1.0)\n",
        "clf.fit(X_train_bow, y_train)\n",
        "\n",
        "# Make predictions on the test set with adjusted threshold\n",
        "threshold = 0.5\n",
        "y_pred_probs = clf.predict_proba(X_test_bow)\n",
        "y_pred_adjusted = (y_pred_probs[:, 1] > threshold).astype(int)\n",
        "\n",
        "# Evaluate the model with the adjusted threshold\n",
        "accuracy = accuracy_score(y_test, y_pred_adjusted)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "report = classification_report(y_test, y_pred_adjusted)\n",
        "print(\"Classification Report:\\n\", report)\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_adjusted)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNqkZPeP3Tov",
        "outputId": "fe5a6f6f-0c25-446a-ca9b-3b268601ffe1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8715886499186698\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.89      0.88      5878\n",
            "           1       0.87      0.85      0.86      5188\n",
            "\n",
            "    accuracy                           0.87     11066\n",
            "   macro avg       0.87      0.87      0.87     11066\n",
            "weighted avg       0.87      0.87      0.87     11066\n",
            "\n",
            "Confusion Matrix:\n",
            " [[5227  651]\n",
            " [ 770 4418]]\n"
          ]
        }
      ]
    }
  ]
}