{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the first dataset\n",
        "df1 = pd.read_json('Sarcasm_Headlines_Dataset.json', lines=True)\n",
        "\n",
        "# Load the second dataset\n",
        "df2 = pd.read_json('Sarcasm_Headlines_Dataset_v2.json', lines=True)\n",
        "\n",
        "# Concatenate both datasets into a single dataframe\n",
        "df = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "# Calculate the total number of headlines\n",
        "total_headlines = len(df)\n",
        "\n",
        "# Calculate the percentage of sarcastic and non-sarcastic headlines\n",
        "num_sarcastic = df['is_sarcastic'].sum()\n",
        "num_non_sarcastic = total_headlines - num_sarcastic\n",
        "percentage_sarcastic = (num_sarcastic / total_headlines) * 100\n",
        "percentage_non_sarcastic = (num_non_sarcastic / total_headlines) * 100\n",
        "\n",
        "# Calculate the average headline length\n",
        "df['headline_length'] = df['headline'].apply(lambda x: len(x.split()))\n",
        "average_headline_length = df['headline_length'].mean()\n",
        "\n",
        "# Find the minimum and maximum headline lengths\n",
        "min_headline_length = df['headline_length'].min()\n",
        "max_headline_length = df['headline_length'].max()\n",
        "\n",
        "# Print the results\n",
        "print(\"Total number of headlines:\", total_headlines)\n",
        "print(\"Percentage of sarcastic headlines:\", percentage_sarcastic, \"%\")\n",
        "print(\"Percentage of non-sarcastic headlines:\", percentage_non_sarcastic, \"%\")\n",
        "print(\"Average headline length:\", average_headline_length, \"words\")\n",
        "print(\"Minimum headline length:\", min_headline_length, \"words\")\n",
        "print(\"Maximum headline length:\", max_headline_length, \"words\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebySbyxs9loO",
        "outputId": "8aaf6bc4-d0a5-4c7f-ef3d-d1ff1697cd54"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of headlines: 55328\n",
            "Percentage of sarcastic headlines: 45.83212839791787 %\n",
            "Percentage of non-sarcastic headlines: 54.16787160208213 %\n",
            "Average headline length: 9.951417004048583 words\n",
            "Minimum headline length: 2 words\n",
            "Maximum headline length: 151 words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the datasets and combine them\n",
        "data_v1 = pd.read_json(\"Sarcasm_Headlines_Dataset.json\", lines=True)\n",
        "data_v2 = pd.read_json(\"Sarcasm_Headlines_Dataset_v2.json\", lines=True)\n",
        "data = pd.concat([data_v1, data_v2])\n",
        "\n",
        "# Data cleaning and preprocessing\n",
        "def clean_text(text):\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize and remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    filtered_text = ' '.join(word for word in words if word not in stop_words)\n",
        "\n",
        "    return filtered_text\n",
        "\n",
        "# Apply data cleaning function to 'headline' column\n",
        "data['headline'] = data['headline'].apply(clean_text)\n",
        "\n",
        "# Split the data into training and testing sets (80% training, 20% testing)\n",
        "X = data['headline']\n",
        "y = data['is_sarcastic']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Bag-of-Words representation of the headlines\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "X_test_bow = vectorizer.transform(X_test)\n",
        "\n",
        "# Train the Multinomial Naive Bayes classifier\n",
        "clf_nb = MultinomialNB(alpha=1.0)\n",
        "clf_nb.fit(X_train_bow, y_train)\n",
        "\n",
        "# Train the Logistic Regression classifier\n",
        "clf_lr = LogisticRegression(max_iter=1000, random_state=42)\n",
        "clf_lr.fit(X_train_bow, y_train)\n",
        "\n",
        "# Train the Support Vector Machine classifier\n",
        "clf_svm = SVC(kernel='linear', random_state=42)\n",
        "clf_svm.fit(X_train_bow, y_train)\n",
        "\n",
        "# Make predictions on the test set with adjusted threshold\n",
        "threshold = 0.5\n",
        "y_pred_probs_nb = clf_nb.predict_proba(X_test_bow)\n",
        "y_pred_probs_lr = clf_lr.predict_proba(X_test_bow)\n",
        "y_pred_probs_svm = clf_svm.decision_function(X_test_bow)\n",
        "y_pred_adjusted_nb = (y_pred_probs_nb[:, 1] > threshold).astype(int)\n",
        "y_pred_adjusted_lr = (y_pred_probs_lr[:, 1] > threshold).astype(int)\n",
        "y_pred_adjusted_svm = (y_pred_probs_svm > threshold).astype(int)\n",
        "\n",
        "# Evaluate the models with the adjusted threshold\n",
        "accuracy_nb = accuracy_score(y_test, y_pred_adjusted_nb)\n",
        "accuracy_lr = accuracy_score(y_test, y_pred_adjusted_lr)\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_adjusted_svm)\n",
        "\n",
        "print(\"Accuracy (Multinomial Naive Bayes):\", accuracy_nb)\n",
        "print(\"Accuracy (Logistic Regression):\", accuracy_lr)\n",
        "print(\"Accuracy (Support Vector Machine):\", accuracy_svm)\n",
        "\n",
        "report_nb = classification_report(y_test, y_pred_adjusted_nb)\n",
        "print(\"Classification Report (Multinomial Naive Bayes):\\n\", report_nb)\n",
        "\n",
        "report_lr = classification_report(y_test, y_pred_adjusted_lr)\n",
        "print(\"Classification Report (Logistic Regression):\\n\", report_lr)\n",
        "\n",
        "report_svm = classification_report(y_test, y_pred_adjusted_svm)\n",
        "print(\"Classification Report (Support Vector Machine):\\n\", report_svm)\n",
        "\n",
        "conf_matrix_nb = confusion_matrix(y_test, y_pred_adjusted_nb)\n",
        "print(\"Confusion Matrix (Multinomial Naive Bayes):\\n\", conf_matrix_nb)\n",
        "\n",
        "conf_matrix_lr = confusion_matrix(y_test, y_pred_adjusted_lr)\n",
        "print(\"Confusion Matrix (Logistic Regression):\\n\", conf_matrix_lr)\n",
        "\n",
        "conf_matrix_svm = confusion_matrix(y_test, y_pred_adjusted_svm)\n",
        "print(\"Confusion Matrix (Support Vector Machine):\\n\", conf_matrix_svm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNqkZPeP3Tov",
        "outputId": "e4fcd992-b9dd-4b28-fd9e-4fe39ec0ae60"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Multinomial Naive Bayes): 0.8715886499186698\n",
            "Accuracy (Logistic Regression): 0.8993312850171697\n",
            "Accuracy (Support Vector Machine): 0.9060184348454726\n",
            "Classification Report (Multinomial Naive Bayes):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.89      0.88      5878\n",
            "           1       0.87      0.85      0.86      5188\n",
            "\n",
            "    accuracy                           0.87     11066\n",
            "   macro avg       0.87      0.87      0.87     11066\n",
            "weighted avg       0.87      0.87      0.87     11066\n",
            "\n",
            "Classification Report (Logistic Regression):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.93      0.91      5878\n",
            "           1       0.91      0.87      0.89      5188\n",
            "\n",
            "    accuracy                           0.90     11066\n",
            "   macro avg       0.90      0.90      0.90     11066\n",
            "weighted avg       0.90      0.90      0.90     11066\n",
            "\n",
            "Classification Report (Support Vector Machine):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.97      0.92      5878\n",
            "           1       0.96      0.83      0.89      5188\n",
            "\n",
            "    accuracy                           0.91     11066\n",
            "   macro avg       0.92      0.90      0.90     11066\n",
            "weighted avg       0.91      0.91      0.91     11066\n",
            "\n",
            "Confusion Matrix (Multinomial Naive Bayes):\n",
            " [[5227  651]\n",
            " [ 770 4418]]\n",
            "Confusion Matrix (Logistic Regression):\n",
            " [[5457  421]\n",
            " [ 693 4495]]\n",
            "Confusion Matrix (Support Vector Machine):\n",
            " [[5710  168]\n",
            " [ 872 4316]]\n"
          ]
        }
      ]
    }
  ]
}